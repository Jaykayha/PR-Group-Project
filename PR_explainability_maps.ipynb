{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g5buHgax9-G",
        "outputId": "40858c36-2700-4a3b-99e7-eed51a4a094e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive # connect to google drive to load the model from google drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhGtWwwK8XMb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f8f5c7-896e-4ad1-e300-c542e5f6b0e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rescaling (Rescaling)       (None, None, None, None)  0         \n",
            "                                                                 \n",
            " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 1280)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1280)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 2562      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,260,546\n",
            "Trainable params: 414,722\n",
            "Non-trainable params: 1,845,824\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# loading in the model from google drive (path varies based on user)\n",
        "\n",
        "from tensorflow import keras\n",
        "model_path = \"/content/drive/MyDrive/UU/Pattern Recognition/PR project/1. Model training/final_model.h5\" # depends on YOUR google drive! change accordingly\n",
        "model = keras.models.load_model(model_path)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "# image to test stuff on\n",
        "PATH_TO_IMAGES = \"/content/drive/MyDrive/UU/Pattern Recognition/PR project/3. Experimental setup/16 resized posters (300x400)\"\n",
        "img_path = \"/content/drive/MyDrive/UU/Pattern Recognition/PR project/3. Experimental setup/16 resized posters (300x400)/6912 (action).jpg\" # example image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-S7XCLeF6ZT"
      },
      "source": [
        "# Make prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_DA_L6h6orD"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "def make_prediction(im_path):\n",
        "  image = Image.open(im_path)\n",
        "  new_image = image.resize((224, 224))\n",
        "  plt.imshow(new_image)\n",
        "  new_image = np.asarray(new_image).reshape(1, 224, 224, 3)\n",
        "\n",
        "  prediction = model.predict(new_image)\n",
        "  print(\"Model output:\", prediction, \"\\nClass prediction:\", np.argmax(prediction))\n",
        "\n",
        "make_prediction(img_path) #this is just an image I uploaded to the colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaB2DTbxKdlU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQyLAP8gF7UT"
      },
      "source": [
        "# SHAP\n",
        "\n",
        "Closely following this tutorial: https://medium.com/@tibastar/explain-the-prediction-for-imagenet-using-shap-468ec5bc9904"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjVpQDl9UVdf"
      },
      "outputs": [],
      "source": [
        "! pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE-8S37GETmm"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
        "from keras.preprocessing import image\n",
        "import requests\n",
        "from skimage.segmentation import slic\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKCKo-JAUp02"
      },
      "outputs": [],
      "source": [
        "# FUNCTION DEFINITIONS\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def img_to_batch(img_path):\n",
        "  # turn image into (1, 224, 224, 3), a single batch\n",
        "  img = Image.open(img_path).resize((224, 224))\n",
        "  # array representation of original image\n",
        "  img_array = np.asarray(img)\n",
        "  img_array.resize(1, 224, 224, 3)\n",
        "\n",
        "  return img_array\n",
        "\n",
        "def mask_image(zs, segmentation, image, background=None):\n",
        "    if background is None:\n",
        "        background = image.mean((0,1))\n",
        "    out = np.zeros((zs.shape[0], image.shape[0], image.shape[1], image.shape[2]))\n",
        "    for i in range(zs.shape[0]):\n",
        "        out[i,:,:,:] = image\n",
        "        for j in range(zs.shape[1]):\n",
        "            if zs[i,j] == 0:\n",
        "                out[i][segmentation == j,:] = background\n",
        "    return out\n",
        "\n",
        "# each z is basically a vector representing which segments are (1) or aren't (0)\n",
        "# part of the current image being predicted on\n",
        "\n",
        "# the function below takes a list of z vectors, passes that to mask image\n",
        "# then mask_image generates a series of predictions corresponding \n",
        "# to EACH z vector, coming back with a group of predictions\n",
        "\n",
        "def f(z): \n",
        "  # this becomes our wrapper model which can predict on a set of z vectors\n",
        "  # the z vector itself is taken as a feature vector it seems (?)\n",
        "    return model2.predict(mask_image(z, segments_slic, img_orig_array, 255))\n",
        "\n",
        "def fill_segmentation(values, segmentation):\n",
        "    out = np.zeros(segmentation.shape)\n",
        "    for i in range(len(values)):\n",
        "        out[segmentation == i] = values[i] # whichever pixels have index i get its value\n",
        "    return out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gObJK55_eLT9"
      },
      "source": [
        "## Define a function that generates SHAP explanation for file path of image\n",
        "\n",
        "It returns TWO different 224x224 activation maps: one for '0' and one for '1' labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSZHTihwcH6X"
      },
      "outputs": [],
      "source": [
        "def predict_and_generate_shap_maps(img_path, return_prediction = False, verbose = True):\n",
        "\n",
        "  # make a color map\n",
        "  from matplotlib.colors import LinearSegmentedColormap\n",
        "  colors = []\n",
        "  for l in np.linspace(1,0,100):\n",
        "      colors.append((245/255,39/255,87/255,l))\n",
        "  for l in np.linspace(0,1,100):\n",
        "      colors.append((24/255,196/255,93/255,l))\n",
        "      \n",
        "  cm = LinearSegmentedColormap.from_list(\"shap\", colors)\n",
        "\n",
        "\n",
        "  # open the image\n",
        "  img = Image.open(img_path).resize((224, 224))\n",
        "\n",
        "  # array representation of original image\n",
        "  img_orig_array_local = np.asarray(img)\n",
        "\n",
        "  # segment the image into 50 superpixels\n",
        "  segments_slic_local = slic(img, n_segments=50, compactness=30, sigma=3)\n",
        "  plt.imshow(segments_slic_local);\n",
        "  plt.axis('off');\n",
        "\n",
        "  # define predict function locally (for this image specifically)\n",
        "  def f(z): \n",
        "  # this becomes our wrapper model which can predict on a set of z vectors\n",
        "  # the z vector itself is taken as a feature vector it seems (?)\n",
        "    return model.predict(mask_image(z, segments_slic_local, img_orig_array_local, 255))\n",
        "\n",
        "  # use Kernel SHAP to explain the network's predictions under varying arrangements of masking\n",
        "  explainer = shap.KernelExplainer(f, np.zeros((1,50)))\n",
        "  with warnings.catch_warnings():\n",
        "      warnings.simplefilter(\"ignore\")\n",
        "      shap_values = explainer.shap_values(np.ones((1,50)), nsamples=1000)\n",
        "\n",
        "  # make prediction on the actual image\n",
        "  preds = model.predict(img_orig_array_local.reshape(1,224,224,3))\n",
        "\n",
        "\n",
        "  if verbose:\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12,4))\n",
        "    inds = preds[0]\n",
        "    axes[0].imshow(img)\n",
        "    axes[0].axis('off')\n",
        "\n",
        "  # max shap value\n",
        "  max_val = np.max([np.max(np.abs(shap_values[i][:,:-1])) for i in range(len(shap_values))])\n",
        "\n",
        "\n",
        "  # define dict of activation maps\n",
        "  activation_maps_dict = {}\n",
        "\n",
        "\n",
        "\n",
        "  for i in range(2): # we plot a shap map for each possible label: 0 and 1\n",
        "      m = fill_segmentation(shap_values[i][0], segments_slic_local) # set segmenetation areas equal to shap values\n",
        "\n",
        "      activation_maps_dict[str(i)] = m # save activation map to activation_maps dictionary\n",
        "\n",
        "      if verbose:\n",
        "        print(m.shape)\n",
        "        axes[i+1].set_title(str(i))\n",
        "        axes[i+1].imshow(img.convert('LA'), alpha=0.3)\n",
        "        im = axes[i+1].imshow(m, cmap=cm, vmin=-max_val, vmax=max_val)\n",
        "        axes[i+1].axis('off')\n",
        "  if verbose:\n",
        "    cb = fig.colorbar(im, ax=axes.ravel().tolist(), label=\"SHAP value\", orientation=\"horizontal\", aspect=60)\n",
        "    cb.outline.set_visible(False)\n",
        "    plt.show()\n",
        "\n",
        "  if return_prediction == True:\n",
        "    return activation_maps_dict, preds\n",
        "  if return_prediction == False:\n",
        "    return activation_maps_dict\n",
        "    \n",
        "# test!\n",
        "# predict_and_generate_shap_maps(img_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKnqZl8hhBRD"
      },
      "source": [
        "Note that the previous code created a dictionary **activation_maps**\n",
        "\n",
        "This dictionary holds two activation maps, for class '0' and class '1'\n",
        "\n",
        "The activation maps are 224x224, they can be directly compared to the user's input, assuming we also scale it to 224x224\n",
        "\n",
        "We have to discuss whether we will only consider positive values and set negative values to 0. This may make sense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcWsLDn0f6td"
      },
      "outputs": [],
      "source": [
        "#activation_maps['1'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt9ogOKPg8rm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4StvLxZFhTf"
      },
      "source": [
        "# SmoothGrad\n",
        "\n",
        "From https://github.com/sicara/tf-explain/tree/master/examples/core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Foh1Fo53Fjm6"
      },
      "outputs": [],
      "source": [
        "!pip install tf-explain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoM1z2LeFsko"
      },
      "outputs": [],
      "source": [
        "from tf_explain.core.smoothgrad import SmoothGrad\n",
        "\n",
        "def generate_smoothgrad_maps(img_path, class_indexes = [0,1]):\n",
        "  grids = [] # it will return these grids\n",
        "\n",
        "  img_batch = img_to_batch(img_path)\n",
        "\n",
        "  explainer = SmoothGrad()\n",
        "\n",
        "  for class_index in class_indexes:\n",
        "\n",
        "    data = img_batch, class_index\n",
        "    # Compute SmoothGrad on model\n",
        "    grid = explainer.explain(data, model, class_index, 40, 2.0)\n",
        "\n",
        "    grids.append(grid)\n",
        "  \n",
        "  return grids # return a grid for each target class index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTjuzIyDYWwj"
      },
      "source": [
        "# Occlusion\n",
        "\n",
        "Documentation: https://github.com/sicara/tf-explain/blob/9d7d1e900ec3e3e4b5338fbc43dfb93539acecc2/tf_explain/core/occlusion_sensitivity.py#L14\n",
        "\n",
        "Explanation from MatLab website: \"The occlusionSensitivity function perturbs small areas of the input by replacing it with an occluding mask, typically a gray square. The mask moves across the image, and the change in probability score for a given class is measured as a function of mask position. You can use this method to highlight which parts of the image are most important to the classification: when that part of the image is occluded, the probability score for the predicted class will fall sharply.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVcC9xITYgk0"
      },
      "outputs": [],
      "source": [
        "!pip install tf-explain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGJ4XUNuYiIN"
      },
      "outputs": [],
      "source": [
        "from tf_explain.core.occlusion_sensitivity import OcclusionSensitivity\n",
        "\n",
        "IMAGE_PATH = img_path\n",
        "\n",
        "def generate_occlusion_maps(img_path, class_indexes = [0,1]):\n",
        "  grids = [] # it will return these grids\n",
        "\n",
        "  img_batch = img_to_batch(img_path)\n",
        "\n",
        "  explainer = OcclusionSensitivity()\n",
        "\n",
        "  for class_index in class_indexes:\n",
        "\n",
        "    data = img_batch, class_index\n",
        "    # Compute Occlusion Sensitivity for a given patch_size\n",
        "    grid = explainer.get_sensitivity_map(model, data[0][0], class_index, 20)\n",
        "\n",
        "    grids.append(grid)\n",
        "  \n",
        "  return grids # return a grid for each target class index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgmhz3RPY5aq"
      },
      "source": [
        "# Generate SHAP and GradCam maps for each image, save dataframe...\n",
        "\n",
        "**Just in case, we also save the attention maps for the \"practice\" image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KS7otviHZOMr"
      },
      "outputs": [],
      "source": [
        "# get list of image files to generate attention maps for\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "all_files = os.listdir(PATH_TO_IMAGES)\n",
        "image_files = []\n",
        "\n",
        "for filename in all_files:\n",
        "  if \"jpg\" in filename:\n",
        "    image_files.append(filename)\n",
        "\n",
        "maps_dict = {\"id\":[], \"filename\":[], \"prediction\":[], \"true\":[], \"shap_0\":[], \"shap_1\":[], \"occlusion_0\":[], \"occlusion_1\":[]}\n",
        "\n",
        "\n",
        "# start filling dictionary with data for each file, including prediction, true value, and activation maps by methodn\n",
        "# activation maps are stored as numpy arrays\n",
        "for filename in image_files:\n",
        "  file_path = os.path.join(PATH_TO_IMAGES, filename)\n",
        "\n",
        "  if \"action\" in filename:\n",
        "    id = filename.split(\"(\")[0] #the id\n",
        "    true_val = 1 # if filename contains \"action\", true_val = 1, otherwise true_val = 0\n",
        "  else:\n",
        "    id = filename.split(\".\")[0] # id is nr before dot\n",
        "    true_val = 0\n",
        "\n",
        "  # generate prediction, and also SHAP attention maps for each possible class label\n",
        "  activation_map_dict, preds = predict_and_generate_shap_maps(file_path, return_prediction = True, verbose = False)\n",
        "  shap_0 = activation_map_dict['0']\n",
        "  shap_1 = activation_map_dict['1']\n",
        "\n",
        "  occlusion_0, occlusion_1 = generate_occlusion_maps(file_path, class_indexes = [0,1])\n",
        "\n",
        "  # add to dictionary\n",
        "  maps_dict['id'].append(id)\n",
        "  maps_dict['filename'].append(filename)\n",
        "  maps_dict['prediction'].append(preds)\n",
        "  maps_dict['true'].append(true_val)\n",
        "  maps_dict['shap_0'].append(shap_0)\n",
        "  maps_dict['shap_1'].append(shap_1)\n",
        "  maps_dict['occlusion_0'].append(occlusion_0) # not yet implemented\n",
        "  maps_dict['occlusion_1'].append(occlusion_1)\n",
        "\n",
        "# create dataframe from dictionary\n",
        "maps_df = pd.DataFrame.from_dict(maps_dict)\n",
        "#maps_df.to_csv('newFile.csv', index = False)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maps_df"
      ],
      "metadata": {
        "id": "yyMiVNEkOkpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2OhtVUPE6r6"
      },
      "source": [
        "# Graph all attention maps in dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b24lUtFC6eO"
      },
      "outputs": [],
      "source": [
        "nr_imgs = len(maps_df)\n",
        "fig, axes = plt.subplots(nrows=nr_imgs, ncols=5, figsize = (12,nr_imgs*4));\n",
        "#fig.tight_layout()\n",
        "\n",
        "axes[0, 1].set_title(\"SHAP\\n(not action)\")\n",
        "\n",
        "axes[0, 2].set_title(\"SHAP\\n(action)\")\n",
        "\n",
        "axes[0, 3].set_title(\"Occlusion\\n(not action)\")\n",
        "\n",
        "axes[0, 4].set_title(\"Occlusion\\n(action)\")\n",
        "\n",
        "# make a color map\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "colors = []\n",
        "for l in np.linspace(1,0,100):\n",
        "    colors.append((245/255,39/255,87/255,l))\n",
        "for l in np.linspace(0,1,100):\n",
        "    colors.append((24/255,196/255,93/255,l))\n",
        "    \n",
        "cm = LinearSegmentedColormap.from_list(\"shap\", colors)\n",
        "\n",
        "\n",
        "for index, row in maps_df.iterrows():\n",
        "  img_array = img_to_batch(os.path.join(PATH_TO_IMAGES, row['filename']))[0]\n",
        "  axes[index, 0].imshow(img_array)\n",
        "  axes[index, 0].set_title(f\"Pred {np.argmax(row['prediction'])}\")\n",
        "  \n",
        "  axes[index, 1].matshow(row['shap_0'], cmap=cm)\n",
        "  axes[index, 1].axis('off')\n",
        "\n",
        "  axes[index, 2].matshow(row['shap_1'], cmap=cm)\n",
        "  axes[index, 2].axis('off')\n",
        "\n",
        "  axes[index, 3].matshow(row['occlusion_0'])\n",
        "  axes[index, 3].axis('off')\n",
        "\n",
        "  axes[index, 4].matshow(row['occlusion_1'])\n",
        "  axes[index, 4].axis('off')\n",
        "\n",
        "  # axes[2].matshow(heatmap1)\n",
        "  # axes[2].set_title(\"Action\")\n",
        "  # axes[0].imshow(img_array[0])\n",
        "\n",
        "  plt.savefig('all_maps.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puykLrY1Fr34"
      },
      "outputs": [],
      "source": [
        "maps_df.to_pickle(\"/content/drive/MyDrive/UU/Pattern Recognition/PR project/4. Data analysis/shap_and_occlusion_maps.pickle\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T17ivvHvzNvG"
      },
      "source": [
        "# GradCam (IGNORE)\n",
        "\n",
        "This is pretty much a \"manual\" implementation of GradCam rather than a package. It follows the following tutorial: https://keras.io/examples/vision/grad_cam/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSv6TEflzOem"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Display\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZNa8vZY1PxL"
      },
      "outputs": [],
      "source": [
        "\n",
        "img_size = (224, 224)\n",
        "#preprocess_input = keras.applications.xception.preprocess_input\n",
        "#decode_predictions = keras.applications.xception.decode_predictions\n",
        "\n",
        "last_conv_layer_index = -4 \n",
        "\n",
        "# The local path to our target image\n",
        "img_path = list(uploaded.keys())[0]\n",
        "\n",
        "#display(Image(img_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdQNzzyc1m7K"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "img_to_batch(img_path).shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8G436M32NqB"
      },
      "outputs": [],
      "source": [
        "# function (from tutorial) to create GradCAM heatmap\n",
        "\n",
        "# \\@TINGTING This is my code from my other class but there is some mistake here\n",
        "\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_index, pred_index=None):\n",
        "    # First, we create a model that maps the input image to the activations\n",
        "    # of the last conv layer as well as the output predictions\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], [model.layers[last_conv_layer].output, model.output]\n",
        "    )\n",
        "\n",
        "    # Then, we compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last conv layer\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "        print(grad_model(img_array)[1])\n",
        "        if pred_index is None:\n",
        "            pred_index = np.argmax(preds[0])\n",
        "        class_channel = preds[:, pred_index]\n",
        "\n",
        "    # This is the gradient of the output neuron (top predicted or chosen)\n",
        "    # with regard to the output feature map of the last conv layer\n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    # then sum all the channels to obtain the heatmap class activation\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtkRUZB114gS"
      },
      "outputs": [],
      "source": [
        "img_array = img_to_batch(img_path)\n",
        "heatmap0 = make_gradcam_heatmap(img_array, model, last_conv_layer_index, pred_index = 0)\n",
        "heatmap1 = make_gradcam_heatmap(img_array, model, last_conv_layer_index, pred_index = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELOgVdNsyE1K"
      },
      "outputs": [],
      "source": [
        "# Display heatmap\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12,4))\n",
        "axes[1].matshow(heatmap0)\n",
        "axes[1].set_title(\"Not Action\")\n",
        "axes[2].matshow(heatmap1)\n",
        "axes[2].set_title(\"Action\")\n",
        "axes[0].imshow(img_array[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGFpdZwAJxGh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "T17ivvHvzNvG"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}